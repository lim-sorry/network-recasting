{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import matplotlib as plt\n",
    "import time\n",
    "\n",
    "from model import CustomResNet, ConvBlock\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import ToTensor, Compose, RandomCrop, RandomHorizontalFlip, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = 'cifar100'\n",
    "teacher_name = 'resnet38_baseline'\n",
    "student_name = 'resnet38_recast'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_class = 100\n",
    "batch_size = 64\n",
    "epoch = 60\n",
    "lr = 0.001\n",
    "step_size = 20\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(size=[32, 32], padding=4),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR100(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 569972\n",
      "CustomResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layers): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (16): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (17): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "teacher = CustomResNet(block=BasicBlock,\n",
    "                   layers=[6, 6, 6],\n",
    "                   num_classes=num_class).to(device).eval()\n",
    "checkpoint = torch.load(f'./result/{dataset_name}_{teacher_name}.pt')\n",
    "teacher.load_state_dict(checkpoint['model'])\n",
    "\n",
    "total_params = sum(p.numel() for p in teacher.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "print(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 569972\n",
      "CustomResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layers): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (16): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (17): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "student = CustomResNet(block=BasicBlock,\n",
    "                   layers=[6, 6, 6],\n",
    "                   num_classes=num_class).to(device)\n",
    "checkpoint = torch.load(f'./result/{dataset_name}_{teacher_name}.pt')\n",
    "student.load_state_dict(checkpoint['model'])\n",
    "\n",
    "total_params = sum(p.numel() for p in student.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 0 recasting...\n",
      "block 0 training started...\n",
      "epoch=0, train_loss=0.4141, test_acc=0.6315\n",
      "epoch=1, train_loss=0.1112, test_acc=0.6644\n",
      "epoch=2, train_loss=0.0598, test_acc=0.6708\n",
      "epoch=3, train_loss=0.0382, test_acc=0.6739\n",
      "epoch=4, train_loss=0.0318, test_acc=0.6811\n",
      "epoch=5, train_loss=0.0286, test_acc=0.6779\n",
      "epoch=6, train_loss=0.0256, test_acc=0.6819\n",
      "epoch=7, train_loss=0.0237, test_acc=0.6816\n",
      "epoch=8, train_loss=0.0230, test_acc=0.6827\n",
      "epoch=9, train_loss=0.0230, test_acc=0.6830\n",
      "epoch=10, train_loss=0.0231, test_acc=0.6828\n",
      "epoch=11, train_loss=0.0229, test_acc=0.6832\n",
      "epoch=12, train_loss=0.0227, test_acc=0.6815\n",
      "epoch=13, train_loss=0.0227, test_acc=0.6833\n",
      "epoch=14, train_loss=0.0228, test_acc=0.6823\n",
      "epoch=15, train_loss=0.0227, test_acc=0.6801\n",
      "epoch=16, train_loss=0.0228, test_acc=0.6825\n",
      "epoch=17, train_loss=0.0226, test_acc=0.6827\n",
      "epoch=18, train_loss=0.0228, test_acc=0.6832\n",
      "epoch=19, train_loss=0.0227, test_acc=0.6817\n",
      "epoch=20, train_loss=0.0226, test_acc=0.6847\n",
      "epoch=21, train_loss=0.0226, test_acc=0.6826\n",
      "epoch=22, train_loss=0.0224, test_acc=0.6814\n",
      "epoch=23, train_loss=0.0224, test_acc=0.6827\n",
      "epoch=24, train_loss=0.0224, test_acc=0.6823\n",
      "epoch=25, train_loss=0.0224, test_acc=0.6826\n",
      "epoch=26, train_loss=0.0224, test_acc=0.6832\n",
      "epoch=27, train_loss=0.0226, test_acc=0.6838\n",
      "epoch=28, train_loss=0.0224, test_acc=0.6833\n",
      "epoch=29, train_loss=0.0222, test_acc=0.6825\n",
      "epoch=30, train_loss=0.0226, test_acc=0.6836\n",
      "epoch=31, train_loss=0.0225, test_acc=0.6821\n",
      "epoch=32, train_loss=0.0225, test_acc=0.6839\n",
      "epoch=33, train_loss=0.0225, test_acc=0.6834\n",
      "epoch=34, train_loss=0.0224, test_acc=0.6826\n",
      "epoch=35, train_loss=0.0225, test_acc=0.6830\n",
      "epoch=36, train_loss=0.0225, test_acc=0.6820\n",
      "epoch=37, train_loss=0.0223, test_acc=0.6839\n",
      "epoch=38, train_loss=0.0225, test_acc=0.6828\n",
      "epoch=39, train_loss=0.0224, test_acc=0.6814\n",
      "epoch=40, train_loss=0.0226, test_acc=0.6831\n",
      "epoch=41, train_loss=0.0223, test_acc=0.6842\n",
      "epoch=42, train_loss=0.0224, test_acc=0.6828\n",
      "epoch=43, train_loss=0.0223, test_acc=0.6837\n",
      "epoch=44, train_loss=0.0224, test_acc=0.6827\n",
      "epoch=45, train_loss=0.0223, test_acc=0.6827\n",
      "epoch=46, train_loss=0.0224, test_acc=0.6814\n",
      "epoch=47, train_loss=0.0224, test_acc=0.6837\n",
      "epoch=48, train_loss=0.0224, test_acc=0.6839\n",
      "epoch=49, train_loss=0.0223, test_acc=0.6828\n",
      "epoch=50, train_loss=0.0224, test_acc=0.6838\n",
      "epoch=51, train_loss=0.0225, test_acc=0.6822\n",
      "epoch=52, train_loss=0.0225, test_acc=0.6844\n",
      "epoch=53, train_loss=0.0225, test_acc=0.6829\n",
      "epoch=54, train_loss=0.0223, test_acc=0.6826\n",
      "epoch=55, train_loss=0.0224, test_acc=0.6842\n",
      "epoch=56, train_loss=0.0222, test_acc=0.6829\n",
      "epoch=57, train_loss=0.0224, test_acc=0.6823\n",
      "epoch=58, train_loss=0.0224, test_acc=0.6824\n",
      "epoch=59, train_loss=0.0224, test_acc=0.6824\n",
      "block 1 recasting...\n",
      "block 1 training started...\n",
      "epoch=0, train_loss=0.5093, test_acc=0.6224\n",
      "epoch=1, train_loss=0.1316, test_acc=0.6713\n",
      "epoch=2, train_loss=0.0546, test_acc=0.6811\n",
      "epoch=3, train_loss=0.0354, test_acc=0.6834\n",
      "epoch=4, train_loss=0.0300, test_acc=0.6827\n",
      "epoch=5, train_loss=0.0282, test_acc=0.6839\n",
      "epoch=6, train_loss=0.0270, test_acc=0.6845\n",
      "epoch=7, train_loss=0.0263, test_acc=0.6816\n",
      "epoch=8, train_loss=0.0257, test_acc=0.6858\n",
      "epoch=9, train_loss=0.0253, test_acc=0.6821\n",
      "epoch=10, train_loss=0.0249, test_acc=0.6797\n",
      "epoch=11, train_loss=0.0246, test_acc=0.6851\n",
      "epoch=12, train_loss=0.0244, test_acc=0.6867\n",
      "epoch=13, train_loss=0.0243, test_acc=0.6843\n",
      "epoch=14, train_loss=0.0242, test_acc=0.6854\n",
      "epoch=15, train_loss=0.0240, test_acc=0.6843\n",
      "epoch=16, train_loss=0.0239, test_acc=0.6854\n",
      "epoch=17, train_loss=0.0237, test_acc=0.6870\n",
      "epoch=18, train_loss=0.0235, test_acc=0.6870\n",
      "epoch=19, train_loss=0.0236, test_acc=0.6843\n",
      "epoch=20, train_loss=0.0229, test_acc=0.6861\n",
      "epoch=21, train_loss=0.0227, test_acc=0.6857\n",
      "epoch=22, train_loss=0.0227, test_acc=0.6881\n",
      "epoch=23, train_loss=0.0230, test_acc=0.6874\n",
      "epoch=24, train_loss=0.0229, test_acc=0.6856\n",
      "epoch=25, train_loss=0.0229, test_acc=0.6869\n",
      "epoch=26, train_loss=0.0227, test_acc=0.6869\n",
      "epoch=27, train_loss=0.0229, test_acc=0.6864\n",
      "epoch=28, train_loss=0.0229, test_acc=0.6846\n",
      "epoch=29, train_loss=0.0226, test_acc=0.6858\n",
      "epoch=30, train_loss=0.0228, test_acc=0.6861\n",
      "epoch=31, train_loss=0.0228, test_acc=0.6874\n",
      "epoch=32, train_loss=0.0228, test_acc=0.6850\n",
      "epoch=33, train_loss=0.0228, test_acc=0.6863\n",
      "epoch=34, train_loss=0.0228, test_acc=0.6870\n",
      "epoch=35, train_loss=0.0226, test_acc=0.6850\n",
      "epoch=36, train_loss=0.0228, test_acc=0.6862\n",
      "epoch=37, train_loss=0.0227, test_acc=0.6868\n",
      "epoch=38, train_loss=0.0228, test_acc=0.6863\n",
      "epoch=39, train_loss=0.0227, test_acc=0.6875\n",
      "epoch=40, train_loss=0.0227, test_acc=0.6862\n",
      "epoch=41, train_loss=0.0226, test_acc=0.6866\n",
      "epoch=42, train_loss=0.0226, test_acc=0.6858\n",
      "epoch=43, train_loss=0.0227, test_acc=0.6867\n",
      "epoch=44, train_loss=0.0226, test_acc=0.6871\n",
      "epoch=45, train_loss=0.0226, test_acc=0.6852\n",
      "epoch=46, train_loss=0.0226, test_acc=0.6874\n",
      "epoch=47, train_loss=0.0227, test_acc=0.6865\n",
      "epoch=48, train_loss=0.0227, test_acc=0.6871\n",
      "epoch=49, train_loss=0.0227, test_acc=0.6862\n",
      "epoch=50, train_loss=0.0228, test_acc=0.6851\n",
      "epoch=51, train_loss=0.0226, test_acc=0.6873\n",
      "epoch=52, train_loss=0.0226, test_acc=0.6867\n",
      "epoch=53, train_loss=0.0226, test_acc=0.6868\n",
      "epoch=54, train_loss=0.0227, test_acc=0.6856\n",
      "epoch=55, train_loss=0.0227, test_acc=0.6882\n",
      "epoch=56, train_loss=0.0226, test_acc=0.6867\n",
      "epoch=57, train_loss=0.0226, test_acc=0.6857\n",
      "epoch=58, train_loss=0.0226, test_acc=0.6866\n",
      "epoch=59, train_loss=0.0227, test_acc=0.6865\n",
      "block 2 recasting...\n",
      "block 2 training started...\n",
      "epoch=0, train_loss=0.6626, test_acc=0.5966\n",
      "epoch=1, train_loss=0.1753, test_acc=0.6662\n",
      "epoch=2, train_loss=0.0784, test_acc=0.6763\n",
      "epoch=3, train_loss=0.0494, test_acc=0.6820\n",
      "epoch=4, train_loss=0.0387, test_acc=0.6814\n",
      "epoch=5, train_loss=0.0357, test_acc=0.6776\n",
      "epoch=6, train_loss=0.0321, test_acc=0.6835\n",
      "epoch=7, train_loss=0.0307, test_acc=0.6840\n",
      "epoch=8, train_loss=0.0294, test_acc=0.6849\n",
      "epoch=9, train_loss=0.0282, test_acc=0.6838\n",
      "epoch=10, train_loss=0.0276, test_acc=0.6854\n",
      "epoch=11, train_loss=0.0271, test_acc=0.6832\n",
      "epoch=12, train_loss=0.0266, test_acc=0.6845\n",
      "epoch=13, train_loss=0.0262, test_acc=0.6838\n",
      "epoch=14, train_loss=0.0256, test_acc=0.6837\n",
      "epoch=15, train_loss=0.0254, test_acc=0.6843\n",
      "epoch=16, train_loss=0.0250, test_acc=0.6828\n",
      "epoch=17, train_loss=0.0252, test_acc=0.6839\n",
      "epoch=18, train_loss=0.0248, test_acc=0.6858\n",
      "epoch=19, train_loss=0.0249, test_acc=0.6851\n",
      "epoch=20, train_loss=0.0241, test_acc=0.6865\n",
      "epoch=21, train_loss=0.0239, test_acc=0.6865\n",
      "epoch=22, train_loss=0.0241, test_acc=0.6862\n",
      "epoch=23, train_loss=0.0240, test_acc=0.6866\n",
      "epoch=24, train_loss=0.0238, test_acc=0.6863\n",
      "epoch=25, train_loss=0.0238, test_acc=0.6864\n",
      "epoch=26, train_loss=0.0239, test_acc=0.6871\n",
      "epoch=27, train_loss=0.0239, test_acc=0.6845\n",
      "epoch=28, train_loss=0.0239, test_acc=0.6870\n",
      "epoch=29, train_loss=0.0239, test_acc=0.6853\n",
      "epoch=30, train_loss=0.0239, test_acc=0.6857\n",
      "epoch=31, train_loss=0.0239, test_acc=0.6857\n",
      "epoch=32, train_loss=0.0238, test_acc=0.6861\n",
      "epoch=33, train_loss=0.0239, test_acc=0.6865\n",
      "epoch=34, train_loss=0.0237, test_acc=0.6863\n",
      "epoch=35, train_loss=0.0238, test_acc=0.6862\n",
      "epoch=36, train_loss=0.0240, test_acc=0.6853\n",
      "epoch=37, train_loss=0.0237, test_acc=0.6858\n",
      "epoch=38, train_loss=0.0238, test_acc=0.6863\n",
      "epoch=39, train_loss=0.0237, test_acc=0.6860\n",
      "epoch=40, train_loss=0.0236, test_acc=0.6861\n",
      "epoch=41, train_loss=0.0236, test_acc=0.6857\n",
      "epoch=42, train_loss=0.0237, test_acc=0.6870\n",
      "epoch=43, train_loss=0.0236, test_acc=0.6843\n",
      "epoch=44, train_loss=0.0236, test_acc=0.6861\n",
      "epoch=45, train_loss=0.0235, test_acc=0.6858\n",
      "epoch=46, train_loss=0.0237, test_acc=0.6860\n",
      "epoch=47, train_loss=0.0236, test_acc=0.6857\n",
      "epoch=48, train_loss=0.0237, test_acc=0.6865\n",
      "epoch=49, train_loss=0.0236, test_acc=0.6859\n",
      "epoch=50, train_loss=0.0236, test_acc=0.6869\n",
      "epoch=51, train_loss=0.0236, test_acc=0.6862\n",
      "epoch=52, train_loss=0.0235, test_acc=0.6861\n",
      "epoch=53, train_loss=0.0237, test_acc=0.6846\n",
      "epoch=54, train_loss=0.0238, test_acc=0.6863\n",
      "epoch=55, train_loss=0.0237, test_acc=0.6855\n",
      "epoch=56, train_loss=0.0236, test_acc=0.6854\n",
      "epoch=57, train_loss=0.0237, test_acc=0.6861\n",
      "epoch=58, train_loss=0.0235, test_acc=0.6879\n",
      "epoch=59, train_loss=0.0237, test_acc=0.6841\n",
      "block 3 recasting...\n",
      "block 3 training started...\n",
      "epoch=0, train_loss=0.9561, test_acc=0.5789\n",
      "epoch=1, train_loss=0.2799, test_acc=0.6608\n",
      "epoch=2, train_loss=0.1297, test_acc=0.6774\n",
      "epoch=3, train_loss=0.0816, test_acc=0.6799\n",
      "epoch=4, train_loss=0.0529, test_acc=0.6838\n",
      "epoch=5, train_loss=0.0448, test_acc=0.6803\n",
      "epoch=6, train_loss=0.0422, test_acc=0.6825\n",
      "epoch=7, train_loss=0.0405, test_acc=0.6813\n",
      "epoch=8, train_loss=0.0405, test_acc=0.6814\n",
      "epoch=9, train_loss=0.0391, test_acc=0.6775\n",
      "epoch=10, train_loss=0.0388, test_acc=0.6813\n",
      "epoch=11, train_loss=0.0381, test_acc=0.6834\n",
      "epoch=12, train_loss=0.0378, test_acc=0.6842\n",
      "epoch=13, train_loss=0.0369, test_acc=0.6787\n",
      "epoch=14, train_loss=0.0364, test_acc=0.6833\n",
      "epoch=15, train_loss=0.0358, test_acc=0.6816\n",
      "epoch=16, train_loss=0.0357, test_acc=0.6808\n",
      "epoch=17, train_loss=0.0357, test_acc=0.6804\n",
      "epoch=18, train_loss=0.0353, test_acc=0.6816\n",
      "epoch=19, train_loss=0.0348, test_acc=0.6831\n",
      "epoch=20, train_loss=0.0336, test_acc=0.6822\n",
      "epoch=21, train_loss=0.0338, test_acc=0.6836\n",
      "epoch=22, train_loss=0.0336, test_acc=0.6828\n",
      "epoch=23, train_loss=0.0334, test_acc=0.6831\n",
      "epoch=24, train_loss=0.0337, test_acc=0.6833\n",
      "epoch=25, train_loss=0.0335, test_acc=0.6836\n",
      "epoch=26, train_loss=0.0334, test_acc=0.6817\n",
      "epoch=27, train_loss=0.0336, test_acc=0.6832\n",
      "epoch=28, train_loss=0.0336, test_acc=0.6841\n",
      "epoch=29, train_loss=0.0335, test_acc=0.6830\n",
      "epoch=30, train_loss=0.0334, test_acc=0.6823\n",
      "epoch=31, train_loss=0.0332, test_acc=0.6840\n",
      "epoch=32, train_loss=0.0331, test_acc=0.6839\n",
      "epoch=33, train_loss=0.0334, test_acc=0.6816\n",
      "epoch=34, train_loss=0.0333, test_acc=0.6817\n",
      "epoch=35, train_loss=0.0331, test_acc=0.6825\n",
      "epoch=36, train_loss=0.0332, test_acc=0.6823\n",
      "epoch=37, train_loss=0.0331, test_acc=0.6800\n",
      "epoch=38, train_loss=0.0331, test_acc=0.6833\n",
      "epoch=39, train_loss=0.0332, test_acc=0.6822\n",
      "epoch=40, train_loss=0.0331, test_acc=0.6833\n",
      "epoch=41, train_loss=0.0330, test_acc=0.6829\n",
      "epoch=42, train_loss=0.0329, test_acc=0.6818\n",
      "epoch=43, train_loss=0.0329, test_acc=0.6820\n",
      "epoch=44, train_loss=0.0330, test_acc=0.6819\n",
      "epoch=45, train_loss=0.0332, test_acc=0.6808\n",
      "epoch=46, train_loss=0.0332, test_acc=0.6811\n",
      "epoch=47, train_loss=0.0332, test_acc=0.6835\n",
      "epoch=48, train_loss=0.0329, test_acc=0.6833\n",
      "epoch=49, train_loss=0.0331, test_acc=0.6806\n",
      "epoch=50, train_loss=0.0332, test_acc=0.6821\n",
      "epoch=51, train_loss=0.0329, test_acc=0.6825\n",
      "epoch=52, train_loss=0.0331, test_acc=0.6822\n",
      "epoch=53, train_loss=0.0330, test_acc=0.6821\n",
      "epoch=54, train_loss=0.0332, test_acc=0.6825\n",
      "epoch=55, train_loss=0.0329, test_acc=0.6832\n",
      "epoch=56, train_loss=0.0331, test_acc=0.6818\n",
      "epoch=57, train_loss=0.0332, test_acc=0.6830\n",
      "epoch=58, train_loss=0.0330, test_acc=0.6827\n",
      "epoch=59, train_loss=0.0330, test_acc=0.6839\n",
      "block 4 recasting...\n",
      "block 4 training started...\n",
      "epoch=0, train_loss=1.3226, test_acc=0.5123\n",
      "epoch=1, train_loss=0.4410, test_acc=0.6434\n",
      "epoch=2, train_loss=0.2161, test_acc=0.6564\n",
      "epoch=3, train_loss=0.1413, test_acc=0.6731\n",
      "epoch=4, train_loss=0.1017, test_acc=0.6768\n",
      "epoch=5, train_loss=0.0803, test_acc=0.6698\n",
      "epoch=6, train_loss=0.0732, test_acc=0.6628\n",
      "epoch=7, train_loss=0.0703, test_acc=0.6677\n",
      "epoch=8, train_loss=0.0688, test_acc=0.6757\n",
      "epoch=9, train_loss=0.0668, test_acc=0.6757\n",
      "epoch=10, train_loss=0.0660, test_acc=0.6816\n",
      "epoch=11, train_loss=0.0646, test_acc=0.6811\n",
      "epoch=12, train_loss=0.0637, test_acc=0.6807\n",
      "epoch=13, train_loss=0.0630, test_acc=0.6783\n",
      "epoch=14, train_loss=0.0604, test_acc=0.6790\n",
      "epoch=15, train_loss=0.0583, test_acc=0.6810\n",
      "epoch=16, train_loss=0.0577, test_acc=0.6798\n",
      "epoch=17, train_loss=0.0575, test_acc=0.6848\n",
      "epoch=18, train_loss=0.0574, test_acc=0.6838\n",
      "epoch=19, train_loss=0.0570, test_acc=0.6796\n",
      "epoch=20, train_loss=0.0550, test_acc=0.6819\n",
      "epoch=21, train_loss=0.0548, test_acc=0.6826\n",
      "epoch=22, train_loss=0.0551, test_acc=0.6816\n",
      "epoch=23, train_loss=0.0554, test_acc=0.6812\n",
      "epoch=24, train_loss=0.0548, test_acc=0.6821\n",
      "epoch=25, train_loss=0.0548, test_acc=0.6808\n",
      "epoch=26, train_loss=0.0546, test_acc=0.6831\n",
      "epoch=27, train_loss=0.0548, test_acc=0.6820\n",
      "epoch=28, train_loss=0.0546, test_acc=0.6804\n",
      "epoch=29, train_loss=0.0545, test_acc=0.6820\n",
      "epoch=30, train_loss=0.0548, test_acc=0.6823\n",
      "epoch=31, train_loss=0.0546, test_acc=0.6825\n",
      "epoch=32, train_loss=0.0544, test_acc=0.6814\n",
      "epoch=33, train_loss=0.0546, test_acc=0.6811\n",
      "epoch=34, train_loss=0.0548, test_acc=0.6821\n",
      "epoch=35, train_loss=0.0548, test_acc=0.6817\n",
      "epoch=36, train_loss=0.0544, test_acc=0.6807\n",
      "epoch=37, train_loss=0.0545, test_acc=0.6826\n",
      "epoch=38, train_loss=0.0543, test_acc=0.6812\n",
      "epoch=39, train_loss=0.0542, test_acc=0.6828\n",
      "epoch=40, train_loss=0.0540, test_acc=0.6830\n",
      "epoch=41, train_loss=0.0541, test_acc=0.6834\n",
      "epoch=42, train_loss=0.0546, test_acc=0.6823\n",
      "epoch=43, train_loss=0.0540, test_acc=0.6828\n",
      "epoch=44, train_loss=0.0539, test_acc=0.6818\n",
      "epoch=45, train_loss=0.0540, test_acc=0.6820\n",
      "epoch=46, train_loss=0.0541, test_acc=0.6807\n",
      "epoch=47, train_loss=0.0542, test_acc=0.6819\n",
      "epoch=48, train_loss=0.0541, test_acc=0.6815\n",
      "epoch=49, train_loss=0.0540, test_acc=0.6821\n",
      "epoch=50, train_loss=0.0539, test_acc=0.6826\n",
      "epoch=51, train_loss=0.0544, test_acc=0.6823\n",
      "epoch=52, train_loss=0.0540, test_acc=0.6816\n",
      "epoch=53, train_loss=0.0548, test_acc=0.6817\n",
      "epoch=54, train_loss=0.0544, test_acc=0.6818\n",
      "epoch=55, train_loss=0.0541, test_acc=0.6813\n",
      "epoch=56, train_loss=0.0542, test_acc=0.6824\n",
      "epoch=57, train_loss=0.0544, test_acc=0.6823\n",
      "epoch=58, train_loss=0.0539, test_acc=0.6819\n",
      "epoch=59, train_loss=0.0542, test_acc=0.6835\n",
      "block 5 recasting...\n",
      "block 5 training started...\n",
      "epoch=0, train_loss=1.7862, test_acc=0.4580\n",
      "epoch=1, train_loss=0.6511, test_acc=0.6168\n",
      "epoch=2, train_loss=0.3056, test_acc=0.6432\n",
      "epoch=3, train_loss=0.1984, test_acc=0.6662\n",
      "epoch=4, train_loss=0.1598, test_acc=0.6706\n",
      "epoch=5, train_loss=0.1307, test_acc=0.6768\n",
      "epoch=6, train_loss=0.1173, test_acc=0.6625\n",
      "epoch=7, train_loss=0.1068, test_acc=0.6757\n",
      "epoch=8, train_loss=0.0940, test_acc=0.6790\n",
      "epoch=9, train_loss=0.0893, test_acc=0.6728\n",
      "epoch=10, train_loss=0.0873, test_acc=0.6786\n",
      "epoch=11, train_loss=0.0869, test_acc=0.6778\n",
      "epoch=12, train_loss=0.0858, test_acc=0.6766\n",
      "epoch=13, train_loss=0.0836, test_acc=0.6770\n",
      "epoch=14, train_loss=0.0825, test_acc=0.6729\n",
      "epoch=15, train_loss=0.0815, test_acc=0.6785\n",
      "epoch=16, train_loss=0.0808, test_acc=0.6722\n",
      "epoch=17, train_loss=0.0801, test_acc=0.6801\n",
      "epoch=18, train_loss=0.0793, test_acc=0.6759\n",
      "epoch=19, train_loss=0.0782, test_acc=0.6783\n",
      "epoch=20, train_loss=0.0767, test_acc=0.6805\n",
      "epoch=21, train_loss=0.0765, test_acc=0.6807\n",
      "epoch=22, train_loss=0.0762, test_acc=0.6834\n",
      "epoch=23, train_loss=0.0763, test_acc=0.6819\n",
      "epoch=24, train_loss=0.0759, test_acc=0.6817\n",
      "epoch=25, train_loss=0.0762, test_acc=0.6828\n",
      "epoch=26, train_loss=0.0757, test_acc=0.6829\n",
      "epoch=27, train_loss=0.0757, test_acc=0.6822\n",
      "epoch=28, train_loss=0.0756, test_acc=0.6825\n",
      "epoch=29, train_loss=0.0756, test_acc=0.6823\n",
      "epoch=30, train_loss=0.0754, test_acc=0.6824\n",
      "epoch=31, train_loss=0.0750, test_acc=0.6797\n",
      "epoch=32, train_loss=0.0757, test_acc=0.6815\n",
      "epoch=33, train_loss=0.0750, test_acc=0.6827\n",
      "epoch=34, train_loss=0.0755, test_acc=0.6842\n",
      "epoch=35, train_loss=0.0752, test_acc=0.6826\n",
      "epoch=36, train_loss=0.0749, test_acc=0.6817\n",
      "epoch=37, train_loss=0.0758, test_acc=0.6827\n",
      "epoch=38, train_loss=0.0746, test_acc=0.6825\n",
      "epoch=39, train_loss=0.0754, test_acc=0.6828\n",
      "epoch=40, train_loss=0.0743, test_acc=0.6816\n",
      "epoch=41, train_loss=0.0747, test_acc=0.6830\n",
      "epoch=42, train_loss=0.0745, test_acc=0.6820\n",
      "epoch=43, train_loss=0.0747, test_acc=0.6822\n",
      "epoch=44, train_loss=0.0748, test_acc=0.6824\n",
      "epoch=45, train_loss=0.0742, test_acc=0.6830\n",
      "epoch=46, train_loss=0.0748, test_acc=0.6833\n",
      "epoch=47, train_loss=0.0746, test_acc=0.6832\n",
      "epoch=48, train_loss=0.0743, test_acc=0.6830\n",
      "epoch=49, train_loss=0.0743, test_acc=0.6822\n",
      "epoch=50, train_loss=0.0747, test_acc=0.6834\n",
      "epoch=51, train_loss=0.0745, test_acc=0.6811\n",
      "epoch=52, train_loss=0.0746, test_acc=0.6836\n",
      "epoch=53, train_loss=0.0744, test_acc=0.6812\n",
      "epoch=54, train_loss=0.0743, test_acc=0.6819\n",
      "epoch=55, train_loss=0.0744, test_acc=0.6826\n",
      "epoch=56, train_loss=0.0743, test_acc=0.6833\n",
      "epoch=57, train_loss=0.0747, test_acc=0.6821\n",
      "epoch=58, train_loss=0.0743, test_acc=0.6818\n",
      "epoch=59, train_loss=0.0751, test_acc=0.6821\n",
      "block 6 recasting...\n",
      "block 6 training started...\n",
      "epoch=0, train_loss=0.3249, test_acc=0.6414\n",
      "epoch=1, train_loss=0.1109, test_acc=0.6526\n",
      "epoch=2, train_loss=0.0833, test_acc=0.6319\n",
      "epoch=3, train_loss=0.0728, test_acc=0.6716\n",
      "epoch=4, train_loss=0.0678, test_acc=0.6728\n",
      "epoch=5, train_loss=0.0646, test_acc=0.6600\n",
      "epoch=6, train_loss=0.0623, test_acc=0.6681\n",
      "epoch=7, train_loss=0.0603, test_acc=0.6480\n",
      "epoch=8, train_loss=0.0591, test_acc=0.6772\n",
      "epoch=9, train_loss=0.0581, test_acc=0.6713\n",
      "epoch=10, train_loss=0.0563, test_acc=0.6765\n",
      "epoch=11, train_loss=0.0556, test_acc=0.6692\n",
      "epoch=12, train_loss=0.0550, test_acc=0.6731\n",
      "epoch=13, train_loss=0.0548, test_acc=0.6705\n",
      "epoch=14, train_loss=0.0542, test_acc=0.6591\n",
      "epoch=15, train_loss=0.0539, test_acc=0.6787\n",
      "epoch=16, train_loss=0.0537, test_acc=0.6414\n",
      "epoch=17, train_loss=0.0533, test_acc=0.6749\n",
      "epoch=18, train_loss=0.0530, test_acc=0.6617\n",
      "epoch=19, train_loss=0.0526, test_acc=0.6741\n",
      "epoch=20, train_loss=0.0514, test_acc=0.6804\n",
      "epoch=21, train_loss=0.0515, test_acc=0.6787\n",
      "epoch=22, train_loss=0.0513, test_acc=0.6807\n",
      "epoch=23, train_loss=0.0512, test_acc=0.6834\n",
      "epoch=24, train_loss=0.0511, test_acc=0.6795\n",
      "epoch=25, train_loss=0.0510, test_acc=0.6806\n",
      "epoch=26, train_loss=0.0511, test_acc=0.6779\n",
      "epoch=27, train_loss=0.0512, test_acc=0.6758\n",
      "epoch=28, train_loss=0.0512, test_acc=0.6794\n",
      "epoch=29, train_loss=0.0511, test_acc=0.6815\n",
      "epoch=30, train_loss=0.0509, test_acc=0.6807\n",
      "epoch=31, train_loss=0.0510, test_acc=0.6801\n",
      "epoch=32, train_loss=0.0512, test_acc=0.6844\n",
      "epoch=33, train_loss=0.0509, test_acc=0.6787\n",
      "epoch=34, train_loss=0.0508, test_acc=0.6782\n",
      "epoch=35, train_loss=0.0509, test_acc=0.6832\n",
      "epoch=36, train_loss=0.0509, test_acc=0.6785\n",
      "epoch=37, train_loss=0.0508, test_acc=0.6822\n",
      "epoch=38, train_loss=0.0509, test_acc=0.6730\n",
      "epoch=39, train_loss=0.0509, test_acc=0.6789\n",
      "epoch=40, train_loss=0.0506, test_acc=0.6822\n",
      "epoch=41, train_loss=0.0506, test_acc=0.6823\n",
      "epoch=42, train_loss=0.0506, test_acc=0.6807\n",
      "epoch=43, train_loss=0.0505, test_acc=0.6811\n",
      "epoch=44, train_loss=0.0505, test_acc=0.6814\n",
      "epoch=45, train_loss=0.0507, test_acc=0.6823\n",
      "epoch=46, train_loss=0.0505, test_acc=0.6833\n",
      "epoch=47, train_loss=0.0506, test_acc=0.6822\n",
      "epoch=48, train_loss=0.0505, test_acc=0.6817\n",
      "epoch=49, train_loss=0.0506, test_acc=0.6822\n",
      "epoch=50, train_loss=0.0506, test_acc=0.6802\n",
      "epoch=51, train_loss=0.0505, test_acc=0.6838\n",
      "epoch=52, train_loss=0.0504, test_acc=0.6811\n",
      "epoch=53, train_loss=0.0507, test_acc=0.6804\n",
      "epoch=54, train_loss=0.0507, test_acc=0.6805\n",
      "epoch=55, train_loss=0.0505, test_acc=0.6809\n",
      "epoch=56, train_loss=0.0505, test_acc=0.6824\n",
      "epoch=57, train_loss=0.0505, test_acc=0.6813\n",
      "epoch=58, train_loss=0.0506, test_acc=0.6833\n",
      "epoch=59, train_loss=0.0505, test_acc=0.6795\n",
      "block 7 recasting...\n",
      "block 7 training started...\n",
      "epoch=0, train_loss=0.4645, test_acc=0.5678\n",
      "epoch=1, train_loss=0.1593, test_acc=0.6520\n",
      "epoch=2, train_loss=0.1177, test_acc=0.6665\n",
      "epoch=3, train_loss=0.1048, test_acc=0.6676\n",
      "epoch=4, train_loss=0.0999, test_acc=0.6676\n",
      "epoch=5, train_loss=0.0958, test_acc=0.6531\n",
      "epoch=6, train_loss=0.0935, test_acc=0.6626\n",
      "epoch=7, train_loss=0.0917, test_acc=0.6596\n",
      "epoch=8, train_loss=0.0906, test_acc=0.6490\n",
      "epoch=9, train_loss=0.0894, test_acc=0.6702\n",
      "epoch=10, train_loss=0.0885, test_acc=0.6696\n",
      "epoch=11, train_loss=0.0880, test_acc=0.6612\n",
      "epoch=12, train_loss=0.0870, test_acc=0.6739\n",
      "epoch=13, train_loss=0.0865, test_acc=0.6726\n",
      "epoch=14, train_loss=0.0861, test_acc=0.6342\n",
      "epoch=15, train_loss=0.0856, test_acc=0.6469\n",
      "epoch=16, train_loss=0.0848, test_acc=0.6703\n",
      "epoch=17, train_loss=0.0846, test_acc=0.6682\n",
      "epoch=18, train_loss=0.0842, test_acc=0.6761\n",
      "epoch=19, train_loss=0.0839, test_acc=0.6730\n",
      "epoch=20, train_loss=0.0817, test_acc=0.6745\n",
      "epoch=21, train_loss=0.0817, test_acc=0.6767\n",
      "epoch=22, train_loss=0.0815, test_acc=0.6806\n",
      "epoch=23, train_loss=0.0814, test_acc=0.6763\n",
      "epoch=24, train_loss=0.0815, test_acc=0.6775\n",
      "epoch=25, train_loss=0.0816, test_acc=0.6779\n",
      "epoch=26, train_loss=0.0815, test_acc=0.6785\n",
      "epoch=27, train_loss=0.0814, test_acc=0.6774\n",
      "epoch=28, train_loss=0.0815, test_acc=0.6778\n",
      "epoch=29, train_loss=0.0814, test_acc=0.6763\n",
      "epoch=30, train_loss=0.0813, test_acc=0.6775\n",
      "epoch=31, train_loss=0.0812, test_acc=0.6750\n",
      "epoch=32, train_loss=0.0813, test_acc=0.6767\n",
      "epoch=33, train_loss=0.0811, test_acc=0.6789\n",
      "epoch=34, train_loss=0.0811, test_acc=0.6770\n",
      "epoch=35, train_loss=0.0812, test_acc=0.6761\n",
      "epoch=36, train_loss=0.0812, test_acc=0.6771\n",
      "epoch=37, train_loss=0.0811, test_acc=0.6801\n",
      "epoch=38, train_loss=0.0809, test_acc=0.6765\n",
      "epoch=39, train_loss=0.0809, test_acc=0.6766\n",
      "epoch=40, train_loss=0.0807, test_acc=0.6782\n",
      "epoch=41, train_loss=0.0808, test_acc=0.6740\n",
      "epoch=42, train_loss=0.0807, test_acc=0.6779\n",
      "epoch=43, train_loss=0.0807, test_acc=0.6768\n",
      "epoch=44, train_loss=0.0807, test_acc=0.6786\n",
      "epoch=45, train_loss=0.0808, test_acc=0.6757\n",
      "epoch=46, train_loss=0.0808, test_acc=0.6776\n",
      "epoch=47, train_loss=0.0807, test_acc=0.6765\n",
      "epoch=48, train_loss=0.0808, test_acc=0.6786\n",
      "epoch=49, train_loss=0.0808, test_acc=0.6778\n",
      "epoch=50, train_loss=0.0808, test_acc=0.6781\n",
      "epoch=51, train_loss=0.0806, test_acc=0.6776\n",
      "epoch=52, train_loss=0.0809, test_acc=0.6779\n",
      "epoch=53, train_loss=0.0806, test_acc=0.6772\n",
      "epoch=54, train_loss=0.0808, test_acc=0.6785\n",
      "epoch=55, train_loss=0.0807, test_acc=0.6783\n",
      "epoch=56, train_loss=0.0806, test_acc=0.6769\n",
      "epoch=57, train_loss=0.0807, test_acc=0.6772\n",
      "epoch=58, train_loss=0.0807, test_acc=0.6779\n",
      "epoch=59, train_loss=0.0807, test_acc=0.6786\n",
      "block 8 recasting...\n",
      "block 8 training started...\n",
      "epoch=0, train_loss=0.5929, test_acc=0.5620\n",
      "epoch=1, train_loss=0.2199, test_acc=0.6440\n",
      "epoch=2, train_loss=0.1618, test_acc=0.6161\n",
      "epoch=3, train_loss=0.1395, test_acc=0.6552\n",
      "epoch=4, train_loss=0.1328, test_acc=0.6363\n",
      "epoch=5, train_loss=0.1294, test_acc=0.6566\n",
      "epoch=6, train_loss=0.1270, test_acc=0.6462\n",
      "epoch=7, train_loss=0.1252, test_acc=0.6418\n",
      "epoch=8, train_loss=0.1237, test_acc=0.6601\n",
      "epoch=9, train_loss=0.1230, test_acc=0.6374\n",
      "epoch=10, train_loss=0.1219, test_acc=0.6478\n",
      "epoch=11, train_loss=0.1211, test_acc=0.6380\n",
      "epoch=12, train_loss=0.1204, test_acc=0.6573\n",
      "epoch=13, train_loss=0.1196, test_acc=0.6672\n",
      "epoch=14, train_loss=0.1192, test_acc=0.6575\n",
      "epoch=15, train_loss=0.1180, test_acc=0.6605\n",
      "epoch=16, train_loss=0.1175, test_acc=0.6645\n",
      "epoch=17, train_loss=0.1166, test_acc=0.6579\n",
      "epoch=18, train_loss=0.1160, test_acc=0.6601\n",
      "epoch=19, train_loss=0.1157, test_acc=0.6656\n",
      "epoch=20, train_loss=0.1134, test_acc=0.6763\n",
      "epoch=21, train_loss=0.1133, test_acc=0.6743\n",
      "epoch=22, train_loss=0.1132, test_acc=0.6775\n",
      "epoch=23, train_loss=0.1131, test_acc=0.6757\n",
      "epoch=24, train_loss=0.1129, test_acc=0.6709\n",
      "epoch=25, train_loss=0.1131, test_acc=0.6756\n",
      "epoch=26, train_loss=0.1132, test_acc=0.6727\n",
      "epoch=27, train_loss=0.1130, test_acc=0.6778\n",
      "epoch=28, train_loss=0.1130, test_acc=0.6751\n",
      "epoch=29, train_loss=0.1127, test_acc=0.6750\n",
      "epoch=30, train_loss=0.1128, test_acc=0.6757\n",
      "epoch=31, train_loss=0.1126, test_acc=0.6728\n",
      "epoch=32, train_loss=0.1125, test_acc=0.6806\n",
      "epoch=33, train_loss=0.1126, test_acc=0.6766\n",
      "epoch=34, train_loss=0.1126, test_acc=0.6766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m target \u001b[38;5;241m=\u001b[39m teacher(image, idx)\n\u001b[1;32m---> 36\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mstudent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss(pred, target)\n\u001b[0;32m     39\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\js-win-lab\\repo\\network-recasting\\model.py:166\u001b[0m, in \u001b[0;36mCustomResNet.forward\u001b[1;34m(self, x, block_idx)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:Tensor, block_idx:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js-win-lab\\repo\\network-recasting\\model.py:156\u001b[0m, in \u001b[0;36mCustomResNet._forward_impl\u001b[1;34m(self, x, block_idx)\u001b[0m\n\u001b[0;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n\u001b[0;32m    159\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\js-win-lab\\repo\\network-recasting\\model.py:37\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 37\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     39\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = dict()\n",
    "for idx in range(len(student.layers)):\n",
    "    history[idx] = dict(train_loss=[], test_acc=[], train_time=[], test_time=[])\n",
    "    \n",
    "    # recasting\n",
    "    print(f'block {idx} recasting...')\n",
    "    target_block = student.layers[idx]\n",
    "    in_channels = target_block.conv1.in_channels\n",
    "    stride = target_block.conv1.stride\n",
    "    out_channels = target_block.conv2.out_channels\n",
    "    \n",
    "    student.layers[idx] = ConvBlock(in_channels, out_channels, stride).to(device)\n",
    "    for m in student.layers[idx].modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # next block recasting\n",
    "    if idx < len(student.layers)-1:\n",
    "        print(f'block {idx+1} recasting...')\n",
    "        target_block = student.layers[idx]\n",
    "        in_channels = target_block.conv1.in_channels\n",
    "        stride = target_block.conv1.stride\n",
    "        out_channels = target_block.conv2.out_channels\n",
    "\n",
    "    params = []\n",
    "    for i in range(idx + 1):\n",
    "        params.extend(student.layers[i].parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size)\n",
    "\n",
    "    print(f'block {idx} training started...')\n",
    "    for ep in range(epoch):\n",
    "        # train step\n",
    "        train_loss = 0.0\n",
    "        student.train()\n",
    "        s_time = time.time()\n",
    "        for i, (image, _) in enumerate(train_loader):\n",
    "            image = image.to(device)\n",
    "\n",
    "            target = teacher(image, idx)\n",
    "            pred = student(image, idx)\n",
    "\n",
    "            loss = mse_loss(pred, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        e_time = time.time()\n",
    "        history[idx]['train_loss'].append(train_loss/len(train_loader))\n",
    "        history[idx]['train_time'].append(e_time - s_time)\n",
    "\n",
    "        # test step\n",
    "        test_acc = 0.0\n",
    "        student.eval()\n",
    "        s_time = time.time()\n",
    "        for image, target in test_loader:\n",
    "            image = image.to(device)\n",
    "            target = f.one_hot(target, num_class).float().to(device)\n",
    "\n",
    "            pred = student(image)\n",
    "            test_acc += torch.sum(torch.argmax(pred, dim=1) == torch.argmax(target, dim=1)).item()\n",
    "        e_time = time.time()\n",
    "        history[idx]['test_acc'].append(test_acc/len(test_dataset))\n",
    "        history[idx]['test_time'].append(e_time - s_time)\n",
    "        print(f'epoch={ep:d}, train_loss={train_loss/len(train_loader):.4f}, test_acc={test_acc/len(test_dataset):.4f}')\n",
    "\n",
    "        checkpoint = dict(\n",
    "            model=student.state_dict(),\n",
    "            optimizer=optimizer.state_dict(),\n",
    "            history=history,\n",
    "            epoch=ep\n",
    "        )\n",
    "        torch.save(checkpoint, f'./result/{dataset_name}_{student_name}.pt')\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomResNet:\n\tUnexpected key(s) in state_dict: \"layers.9.conv2.weight\", \"layers.9.bn2.weight\", \"layers.9.bn2.bias\", \"layers.9.bn2.running_mean\", \"layers.9.bn2.running_var\", \"layers.9.bn2.num_batches_tracked\", \"layers.10.conv2.weight\", \"layers.10.bn2.weight\", \"layers.10.bn2.bias\", \"layers.10.bn2.running_mean\", \"layers.10.bn2.running_var\", \"layers.10.bn2.num_batches_tracked\", \"layers.11.conv2.weight\", \"layers.11.bn2.weight\", \"layers.11.bn2.bias\", \"layers.11.bn2.running_mean\", \"layers.11.bn2.running_var\", \"layers.11.bn2.num_batches_tracked\", \"layers.12.conv2.weight\", \"layers.12.bn2.weight\", \"layers.12.bn2.bias\", \"layers.12.bn2.running_mean\", \"layers.12.bn2.running_var\", \"layers.12.bn2.num_batches_tracked\", \"layers.12.downsample.0.weight\", \"layers.12.downsample.1.weight\", \"layers.12.downsample.1.bias\", \"layers.12.downsample.1.running_mean\", \"layers.12.downsample.1.running_var\", \"layers.12.downsample.1.num_batches_tracked\", \"layers.13.conv2.weight\", \"layers.13.bn2.weight\", \"layers.13.bn2.bias\", \"layers.13.bn2.running_mean\", \"layers.13.bn2.running_var\", \"layers.13.bn2.num_batches_tracked\", \"layers.14.conv2.weight\", \"layers.14.bn2.weight\", \"layers.14.bn2.bias\", \"layers.14.bn2.running_mean\", \"layers.14.bn2.running_var\", \"layers.14.bn2.num_batches_tracked\", \"layers.15.conv2.weight\", \"layers.15.bn2.weight\", \"layers.15.bn2.bias\", \"layers.15.bn2.running_mean\", \"layers.15.bn2.running_var\", \"layers.15.bn2.num_batches_tracked\", \"layers.16.conv2.weight\", \"layers.16.bn2.weight\", \"layers.16.bn2.bias\", \"layers.16.bn2.running_mean\", \"layers.16.bn2.running_var\", \"layers.16.bn2.num_batches_tracked\", \"layers.17.conv2.weight\", \"layers.17.bn2.weight\", \"layers.17.bn2.bias\", \"layers.17.bn2.running_mean\", \"layers.17.bn2.running_var\", \"layers.17.bn2.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m student \u001b[38;5;241m=\u001b[39m CustomResNet(block\u001b[38;5;241m=\u001b[39mConvBlock,\n\u001b[0;32m      5\u001b[0m                    layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m],\n\u001b[0;32m      6\u001b[0m                    num_classes\u001b[38;5;241m=\u001b[39mnum_class)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mstudent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m student\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal number of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomResNet:\n\tUnexpected key(s) in state_dict: \"layers.9.conv2.weight\", \"layers.9.bn2.weight\", \"layers.9.bn2.bias\", \"layers.9.bn2.running_mean\", \"layers.9.bn2.running_var\", \"layers.9.bn2.num_batches_tracked\", \"layers.10.conv2.weight\", \"layers.10.bn2.weight\", \"layers.10.bn2.bias\", \"layers.10.bn2.running_mean\", \"layers.10.bn2.running_var\", \"layers.10.bn2.num_batches_tracked\", \"layers.11.conv2.weight\", \"layers.11.bn2.weight\", \"layers.11.bn2.bias\", \"layers.11.bn2.running_mean\", \"layers.11.bn2.running_var\", \"layers.11.bn2.num_batches_tracked\", \"layers.12.conv2.weight\", \"layers.12.bn2.weight\", \"layers.12.bn2.bias\", \"layers.12.bn2.running_mean\", \"layers.12.bn2.running_var\", \"layers.12.bn2.num_batches_tracked\", \"layers.12.downsample.0.weight\", \"layers.12.downsample.1.weight\", \"layers.12.downsample.1.bias\", \"layers.12.downsample.1.running_mean\", \"layers.12.downsample.1.running_var\", \"layers.12.downsample.1.num_batches_tracked\", \"layers.13.conv2.weight\", \"layers.13.bn2.weight\", \"layers.13.bn2.bias\", \"layers.13.bn2.running_mean\", \"layers.13.bn2.running_var\", \"layers.13.bn2.num_batches_tracked\", \"layers.14.conv2.weight\", \"layers.14.bn2.weight\", \"layers.14.bn2.bias\", \"layers.14.bn2.running_mean\", \"layers.14.bn2.running_var\", \"layers.14.bn2.num_batches_tracked\", \"layers.15.conv2.weight\", \"layers.15.bn2.weight\", \"layers.15.bn2.bias\", \"layers.15.bn2.running_mean\", \"layers.15.bn2.running_var\", \"layers.15.bn2.num_batches_tracked\", \"layers.16.conv2.weight\", \"layers.16.bn2.weight\", \"layers.16.bn2.bias\", \"layers.16.bn2.running_mean\", \"layers.16.bn2.running_var\", \"layers.16.bn2.num_batches_tracked\", \"layers.17.conv2.weight\", \"layers.17.bn2.weight\", \"layers.17.bn2.bias\", \"layers.17.bn2.running_mean\", \"layers.17.bn2.running_var\", \"layers.17.bn2.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "student = CustomResNet(block=ConvBlock,\n",
    "                   layers=[6, 6, 6],\n",
    "                   num_classes=num_class).to(device)\n",
    "checkpoint = torch.load(f'./result/{dataset_name}_{student_name}.pt')\n",
    "student.load_state_dict(checkpoint['model'])\n",
    "\n",
    "total_params = sum(p.numel() for p in student.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "print(student)\n",
    "\n",
    "checkpoint['history'][0]['test_acc']\n",
    "\n",
    "x = np.arange(len(checkpoint['history']))\n",
    "y = list(checkpoint['history'][i]['test_acc'][-1] for i in range(len(checkpoint['history'])))\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
