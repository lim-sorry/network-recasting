{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import matplotlib as plt\n",
    "import time\n",
    "\n",
    "from model import ConvBlock, CustomResNet, initialize_weights\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import ToTensor, Compose, RandomCrop, RandomHorizontalFlip, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "epoch = 180\n",
    "gamma = 0.1\n",
    "milestones = [90, 120] # from resnet paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(size=[32, 32], padding=4),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 415546\n"
     ]
    }
   ],
   "source": [
    "model_name = 'resnet56_backprop'\n",
    "\n",
    "model = CustomResNet(block=ConvBlock,\n",
    "                   layers=[9, 9, 9],\n",
    "                   num_classes=10).to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\js-win-lab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=1.835, test_acc=0.336\n",
      "epoch=1, train_loss=1.544, test_acc=0.405\n",
      "epoch=2, train_loss=1.310, test_acc=0.540\n",
      "epoch=3, train_loss=1.098, test_acc=0.603\n",
      "epoch=4, train_loss=0.950, test_acc=0.637\n",
      "epoch=5, train_loss=0.852, test_acc=0.631\n",
      "epoch=6, train_loss=0.784, test_acc=0.672\n",
      "epoch=7, train_loss=0.733, test_acc=0.704\n",
      "epoch=8, train_loss=0.692, test_acc=0.723\n",
      "epoch=9, train_loss=0.655, test_acc=0.686\n",
      "epoch=10, train_loss=0.628, test_acc=0.775\n",
      "epoch=11, train_loss=0.603, test_acc=0.736\n",
      "epoch=12, train_loss=0.580, test_acc=0.729\n",
      "epoch=13, train_loss=0.562, test_acc=0.773\n",
      "epoch=14, train_loss=0.545, test_acc=0.717\n",
      "epoch=15, train_loss=0.530, test_acc=0.796\n",
      "epoch=16, train_loss=0.518, test_acc=0.804\n",
      "epoch=17, train_loss=0.503, test_acc=0.768\n",
      "epoch=18, train_loss=0.496, test_acc=0.753\n",
      "epoch=19, train_loss=0.483, test_acc=0.724\n",
      "epoch=20, train_loss=0.479, test_acc=0.806\n",
      "epoch=21, train_loss=0.468, test_acc=0.697\n",
      "epoch=22, train_loss=0.458, test_acc=0.810\n",
      "epoch=23, train_loss=0.456, test_acc=0.725\n",
      "epoch=24, train_loss=0.444, test_acc=0.820\n",
      "epoch=25, train_loss=0.436, test_acc=0.820\n",
      "epoch=26, train_loss=0.432, test_acc=0.779\n",
      "epoch=27, train_loss=0.424, test_acc=0.792\n",
      "epoch=28, train_loss=0.421, test_acc=0.765\n",
      "epoch=29, train_loss=0.418, test_acc=0.786\n",
      "epoch=30, train_loss=0.414, test_acc=0.810\n",
      "epoch=31, train_loss=0.402, test_acc=0.822\n",
      "epoch=32, train_loss=0.402, test_acc=0.806\n",
      "epoch=33, train_loss=0.401, test_acc=0.793\n",
      "epoch=34, train_loss=0.396, test_acc=0.815\n",
      "epoch=35, train_loss=0.387, test_acc=0.807\n",
      "epoch=36, train_loss=0.384, test_acc=0.783\n",
      "epoch=37, train_loss=0.386, test_acc=0.819\n",
      "epoch=38, train_loss=0.382, test_acc=0.792\n",
      "epoch=39, train_loss=0.379, test_acc=0.833\n",
      "epoch=40, train_loss=0.379, test_acc=0.834\n",
      "epoch=41, train_loss=0.370, test_acc=0.836\n",
      "epoch=42, train_loss=0.368, test_acc=0.837\n",
      "epoch=43, train_loss=0.366, test_acc=0.810\n",
      "epoch=44, train_loss=0.362, test_acc=0.742\n",
      "epoch=45, train_loss=0.365, test_acc=0.820\n",
      "epoch=46, train_loss=0.358, test_acc=0.797\n",
      "epoch=47, train_loss=0.358, test_acc=0.834\n",
      "epoch=48, train_loss=0.361, test_acc=0.848\n",
      "epoch=49, train_loss=0.353, test_acc=0.790\n",
      "epoch=50, train_loss=0.352, test_acc=0.814\n",
      "epoch=51, train_loss=0.346, test_acc=0.758\n",
      "epoch=52, train_loss=0.350, test_acc=0.824\n",
      "epoch=53, train_loss=0.344, test_acc=0.811\n",
      "epoch=54, train_loss=0.345, test_acc=0.844\n",
      "epoch=55, train_loss=0.344, test_acc=0.823\n",
      "epoch=56, train_loss=0.343, test_acc=0.816\n",
      "epoch=57, train_loss=0.338, test_acc=0.808\n",
      "epoch=58, train_loss=0.337, test_acc=0.748\n",
      "epoch=59, train_loss=0.333, test_acc=0.820\n",
      "epoch=60, train_loss=0.340, test_acc=0.820\n",
      "epoch=61, train_loss=0.335, test_acc=0.767\n",
      "epoch=62, train_loss=0.334, test_acc=0.806\n",
      "epoch=63, train_loss=0.332, test_acc=0.821\n",
      "epoch=64, train_loss=0.326, test_acc=0.832\n",
      "epoch=65, train_loss=0.329, test_acc=0.847\n",
      "epoch=66, train_loss=0.323, test_acc=0.820\n",
      "epoch=67, train_loss=0.329, test_acc=0.786\n",
      "epoch=68, train_loss=0.325, test_acc=0.781\n",
      "epoch=69, train_loss=0.321, test_acc=0.739\n",
      "epoch=70, train_loss=0.319, test_acc=0.840\n",
      "epoch=71, train_loss=0.323, test_acc=0.853\n",
      "epoch=72, train_loss=0.316, test_acc=0.833\n",
      "epoch=73, train_loss=0.322, test_acc=0.820\n",
      "epoch=74, train_loss=0.319, test_acc=0.847\n",
      "epoch=75, train_loss=0.317, test_acc=0.842\n",
      "epoch=76, train_loss=0.316, test_acc=0.825\n",
      "epoch=77, train_loss=0.318, test_acc=0.848\n",
      "epoch=78, train_loss=0.314, test_acc=0.839\n",
      "epoch=79, train_loss=0.314, test_acc=0.851\n",
      "epoch=80, train_loss=0.313, test_acc=0.848\n",
      "epoch=81, train_loss=0.310, test_acc=0.837\n",
      "epoch=82, train_loss=0.313, test_acc=0.820\n",
      "epoch=83, train_loss=0.309, test_acc=0.829\n",
      "epoch=84, train_loss=0.308, test_acc=0.807\n",
      "epoch=85, train_loss=0.309, test_acc=0.846\n",
      "epoch=86, train_loss=0.307, test_acc=0.769\n",
      "epoch=87, train_loss=0.304, test_acc=0.847\n",
      "epoch=88, train_loss=0.304, test_acc=0.854\n",
      "epoch=89, train_loss=0.305, test_acc=0.837\n",
      "epoch=90, train_loss=0.199, test_acc=0.901\n",
      "epoch=91, train_loss=0.157, test_acc=0.901\n",
      "epoch=92, train_loss=0.147, test_acc=0.902\n",
      "epoch=93, train_loss=0.136, test_acc=0.900\n",
      "epoch=94, train_loss=0.133, test_acc=0.902\n",
      "epoch=95, train_loss=0.126, test_acc=0.903\n",
      "epoch=96, train_loss=0.119, test_acc=0.906\n",
      "epoch=97, train_loss=0.114, test_acc=0.905\n",
      "epoch=98, train_loss=0.115, test_acc=0.901\n",
      "epoch=99, train_loss=0.108, test_acc=0.900\n",
      "epoch=100, train_loss=0.107, test_acc=0.902\n",
      "epoch=101, train_loss=0.101, test_acc=0.903\n",
      "epoch=102, train_loss=0.099, test_acc=0.902\n",
      "epoch=103, train_loss=0.096, test_acc=0.904\n",
      "epoch=104, train_loss=0.092, test_acc=0.904\n",
      "epoch=105, train_loss=0.093, test_acc=0.902\n",
      "epoch=106, train_loss=0.091, test_acc=0.903\n",
      "epoch=107, train_loss=0.088, test_acc=0.904\n",
      "epoch=108, train_loss=0.087, test_acc=0.904\n",
      "epoch=109, train_loss=0.084, test_acc=0.903\n",
      "epoch=110, train_loss=0.081, test_acc=0.903\n",
      "epoch=111, train_loss=0.079, test_acc=0.901\n",
      "epoch=112, train_loss=0.076, test_acc=0.902\n",
      "epoch=113, train_loss=0.074, test_acc=0.899\n",
      "epoch=114, train_loss=0.074, test_acc=0.899\n",
      "epoch=115, train_loss=0.073, test_acc=0.901\n",
      "epoch=116, train_loss=0.071, test_acc=0.902\n",
      "epoch=117, train_loss=0.073, test_acc=0.901\n",
      "epoch=118, train_loss=0.073, test_acc=0.904\n",
      "epoch=119, train_loss=0.070, test_acc=0.903\n",
      "epoch=120, train_loss=0.060, test_acc=0.905\n",
      "epoch=121, train_loss=0.054, test_acc=0.905\n",
      "epoch=122, train_loss=0.053, test_acc=0.906\n",
      "epoch=123, train_loss=0.052, test_acc=0.906\n",
      "epoch=124, train_loss=0.050, test_acc=0.905\n",
      "epoch=125, train_loss=0.047, test_acc=0.904\n",
      "epoch=126, train_loss=0.048, test_acc=0.905\n",
      "epoch=127, train_loss=0.047, test_acc=0.906\n",
      "epoch=128, train_loss=0.047, test_acc=0.905\n",
      "epoch=129, train_loss=0.048, test_acc=0.906\n",
      "epoch=130, train_loss=0.047, test_acc=0.906\n",
      "epoch=131, train_loss=0.045, test_acc=0.905\n",
      "epoch=132, train_loss=0.046, test_acc=0.904\n",
      "epoch=133, train_loss=0.046, test_acc=0.906\n",
      "epoch=134, train_loss=0.045, test_acc=0.905\n",
      "epoch=135, train_loss=0.042, test_acc=0.906\n",
      "epoch=136, train_loss=0.045, test_acc=0.905\n",
      "epoch=137, train_loss=0.045, test_acc=0.906\n",
      "epoch=138, train_loss=0.042, test_acc=0.905\n",
      "epoch=139, train_loss=0.046, test_acc=0.906\n",
      "epoch=140, train_loss=0.043, test_acc=0.906\n",
      "epoch=141, train_loss=0.044, test_acc=0.905\n",
      "epoch=142, train_loss=0.042, test_acc=0.906\n",
      "epoch=143, train_loss=0.042, test_acc=0.905\n",
      "epoch=144, train_loss=0.043, test_acc=0.907\n",
      "epoch=145, train_loss=0.043, test_acc=0.906\n",
      "epoch=146, train_loss=0.040, test_acc=0.905\n",
      "epoch=147, train_loss=0.042, test_acc=0.905\n",
      "epoch=148, train_loss=0.041, test_acc=0.904\n",
      "epoch=149, train_loss=0.041, test_acc=0.906\n",
      "epoch=150, train_loss=0.040, test_acc=0.904\n",
      "epoch=151, train_loss=0.038, test_acc=0.904\n",
      "epoch=152, train_loss=0.039, test_acc=0.904\n",
      "epoch=153, train_loss=0.040, test_acc=0.905\n",
      "epoch=154, train_loss=0.040, test_acc=0.904\n",
      "epoch=155, train_loss=0.040, test_acc=0.905\n",
      "epoch=156, train_loss=0.038, test_acc=0.906\n",
      "epoch=157, train_loss=0.037, test_acc=0.905\n",
      "epoch=158, train_loss=0.038, test_acc=0.904\n",
      "epoch=159, train_loss=0.039, test_acc=0.904\n",
      "epoch=160, train_loss=0.038, test_acc=0.903\n",
      "epoch=161, train_loss=0.039, test_acc=0.904\n",
      "epoch=162, train_loss=0.039, test_acc=0.905\n",
      "epoch=163, train_loss=0.036, test_acc=0.904\n",
      "epoch=164, train_loss=0.037, test_acc=0.905\n",
      "epoch=165, train_loss=0.038, test_acc=0.904\n",
      "epoch=166, train_loss=0.036, test_acc=0.905\n",
      "epoch=167, train_loss=0.036, test_acc=0.904\n",
      "epoch=168, train_loss=0.036, test_acc=0.904\n",
      "epoch=169, train_loss=0.036, test_acc=0.904\n",
      "epoch=170, train_loss=0.037, test_acc=0.906\n",
      "epoch=171, train_loss=0.036, test_acc=0.905\n",
      "epoch=172, train_loss=0.037, test_acc=0.906\n",
      "epoch=173, train_loss=0.035, test_acc=0.905\n",
      "epoch=174, train_loss=0.037, test_acc=0.905\n",
      "epoch=175, train_loss=0.037, test_acc=0.906\n",
      "epoch=176, train_loss=0.037, test_acc=0.905\n",
      "epoch=177, train_loss=0.036, test_acc=0.906\n",
      "epoch=178, train_loss=0.034, test_acc=0.904\n",
      "epoch=179, train_loss=0.035, test_acc=0.905\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(params=model.parameters(), lr=0.1, nesterov=True, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = dict(train_loss=[], test_acc=[], train_time=[], test_time=[])\n",
    "for ep in range(epoch):\n",
    "    # train phase\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    s_time = time.time()\n",
    "    for image, target in train_loader:\n",
    "        image = image.to(device)\n",
    "        target = f.one_hot(target, 10).float().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    e_time = time.time()\n",
    "    history['train_loss'].append(train_loss/len(train_loader))\n",
    "    history['train_time'].append(e_time - s_time)\n",
    "\n",
    "    # test phase\n",
    "    test_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    s_time = time.time()\n",
    "    for image, target in test_loader:\n",
    "        image = image.to(device)\n",
    "        target = f.one_hot(target, 10).float().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        test_acc += torch.sum(torch.argmax(pred, dim=1) == torch.argmax(target, dim=1)).item()\n",
    "    e_time = time.time()\n",
    "    history['test_acc'].append(test_acc/len(test_dataset))\n",
    "    history['test_time'].append(e_time - s_time)\n",
    "    print(f'epoch={ep}, train_loss={train_loss/len(train_loader):.3f}, test_acc={test_acc/len(test_dataset):.3f}')\n",
    "\n",
    "    checkpoint = dict(\n",
    "        model=model.state_dict(),\n",
    "        optimizer=optimizer.state_dict(),\n",
    "        history=history,\n",
    "        epoch=ep\n",
    "    )\n",
    "    torch.save(checkpoint, f'./result/{model_name}.pt')\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
