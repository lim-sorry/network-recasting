{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import matplotlib as plt\n",
    "import time\n",
    "\n",
    "from model import ConvBlock, CustomResNet, initialize_weights\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import ToTensor, Compose, RandomCrop, RandomHorizontalFlip, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "epoch = 180\n",
    "gamma = 0.1\n",
    "milestones = [90, 120] # from resnet paper\n",
    "temp = 5\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(size=[32, 32], padding=4),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 415546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'resnet56_kd'\n",
    "\n",
    "model = CustomResNet(block=ConvBlock,\n",
    "                   layers=[9, 9, 9],\n",
    "                   num_classes=10).to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "\n",
    "t_model = CustomResNet(block=BasicBlock,\n",
    "                   layers=[9, 9, 9],\n",
    "                   num_classes=10).to(device).eval()\n",
    "t_model.load_state_dict(torch.load('./result/resnet56_baseline.pt')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\js-win-lab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "C:\\Users\\js-win-lab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=0.839, test_acc=0.383\n",
      "epoch=1, train_loss=0.704, test_acc=0.460\n",
      "epoch=2, train_loss=0.612, test_acc=0.487\n",
      "epoch=3, train_loss=0.517, test_acc=0.552\n",
      "epoch=4, train_loss=0.438, test_acc=0.631\n",
      "epoch=5, train_loss=0.377, test_acc=0.686\n",
      "epoch=6, train_loss=0.334, test_acc=0.686\n",
      "epoch=7, train_loss=0.302, test_acc=0.690\n",
      "epoch=8, train_loss=0.281, test_acc=0.700\n",
      "epoch=9, train_loss=0.262, test_acc=0.758\n",
      "epoch=10, train_loss=0.245, test_acc=0.746\n",
      "epoch=11, train_loss=0.232, test_acc=0.784\n",
      "epoch=12, train_loss=0.220, test_acc=0.785\n",
      "epoch=13, train_loss=0.207, test_acc=0.747\n",
      "epoch=14, train_loss=0.201, test_acc=0.769\n",
      "epoch=15, train_loss=0.195, test_acc=0.789\n",
      "epoch=16, train_loss=0.188, test_acc=0.746\n",
      "epoch=17, train_loss=0.179, test_acc=0.793\n",
      "epoch=18, train_loss=0.175, test_acc=0.765\n",
      "epoch=19, train_loss=0.172, test_acc=0.798\n",
      "epoch=20, train_loss=0.163, test_acc=0.769\n",
      "epoch=21, train_loss=0.162, test_acc=0.747\n",
      "epoch=22, train_loss=0.158, test_acc=0.730\n",
      "epoch=23, train_loss=0.157, test_acc=0.690\n",
      "epoch=24, train_loss=0.149, test_acc=0.778\n",
      "epoch=25, train_loss=0.149, test_acc=0.757\n",
      "epoch=26, train_loss=0.145, test_acc=0.807\n",
      "epoch=27, train_loss=0.146, test_acc=0.827\n",
      "epoch=28, train_loss=0.140, test_acc=0.818\n",
      "epoch=29, train_loss=0.139, test_acc=0.822\n",
      "epoch=30, train_loss=0.137, test_acc=0.811\n",
      "epoch=31, train_loss=0.134, test_acc=0.751\n",
      "epoch=32, train_loss=0.128, test_acc=0.802\n",
      "epoch=33, train_loss=0.130, test_acc=0.806\n",
      "epoch=34, train_loss=0.127, test_acc=0.828\n",
      "epoch=35, train_loss=0.124, test_acc=0.843\n",
      "epoch=36, train_loss=0.124, test_acc=0.750\n",
      "epoch=37, train_loss=0.122, test_acc=0.816\n",
      "epoch=38, train_loss=0.120, test_acc=0.830\n",
      "epoch=39, train_loss=0.117, test_acc=0.841\n",
      "epoch=40, train_loss=0.116, test_acc=0.782\n",
      "epoch=41, train_loss=0.117, test_acc=0.759\n",
      "epoch=42, train_loss=0.115, test_acc=0.834\n",
      "epoch=43, train_loss=0.112, test_acc=0.826\n",
      "epoch=44, train_loss=0.112, test_acc=0.837\n",
      "epoch=45, train_loss=0.111, test_acc=0.784\n",
      "epoch=46, train_loss=0.107, test_acc=0.832\n",
      "epoch=47, train_loss=0.111, test_acc=0.815\n",
      "epoch=48, train_loss=0.108, test_acc=0.799\n",
      "epoch=49, train_loss=0.105, test_acc=0.734\n",
      "epoch=50, train_loss=0.107, test_acc=0.797\n",
      "epoch=51, train_loss=0.104, test_acc=0.814\n",
      "epoch=52, train_loss=0.106, test_acc=0.846\n",
      "epoch=53, train_loss=0.102, test_acc=0.800\n",
      "epoch=54, train_loss=0.101, test_acc=0.824\n",
      "epoch=55, train_loss=0.103, test_acc=0.793\n",
      "epoch=56, train_loss=0.101, test_acc=0.841\n",
      "epoch=57, train_loss=0.097, test_acc=0.827\n",
      "epoch=58, train_loss=0.099, test_acc=0.846\n",
      "epoch=59, train_loss=0.098, test_acc=0.798\n",
      "epoch=60, train_loss=0.096, test_acc=0.837\n",
      "epoch=61, train_loss=0.094, test_acc=0.832\n",
      "epoch=62, train_loss=0.097, test_acc=0.824\n",
      "epoch=63, train_loss=0.095, test_acc=0.811\n",
      "epoch=64, train_loss=0.093, test_acc=0.759\n",
      "epoch=65, train_loss=0.095, test_acc=0.843\n",
      "epoch=66, train_loss=0.092, test_acc=0.852\n",
      "epoch=67, train_loss=0.092, test_acc=0.830\n",
      "epoch=68, train_loss=0.089, test_acc=0.820\n",
      "epoch=69, train_loss=0.090, test_acc=0.764\n",
      "epoch=70, train_loss=0.089, test_acc=0.830\n",
      "epoch=71, train_loss=0.090, test_acc=0.830\n",
      "epoch=72, train_loss=0.089, test_acc=0.819\n",
      "epoch=73, train_loss=0.088, test_acc=0.825\n",
      "epoch=74, train_loss=0.088, test_acc=0.837\n",
      "epoch=75, train_loss=0.087, test_acc=0.807\n",
      "epoch=76, train_loss=0.088, test_acc=0.811\n",
      "epoch=77, train_loss=0.086, test_acc=0.807\n",
      "epoch=78, train_loss=0.086, test_acc=0.842\n",
      "epoch=79, train_loss=0.086, test_acc=0.793\n",
      "epoch=80, train_loss=0.086, test_acc=0.815\n",
      "epoch=81, train_loss=0.086, test_acc=0.848\n",
      "epoch=82, train_loss=0.083, test_acc=0.786\n",
      "epoch=83, train_loss=0.082, test_acc=0.837\n",
      "epoch=84, train_loss=0.082, test_acc=0.858\n",
      "epoch=85, train_loss=0.079, test_acc=0.829\n",
      "epoch=86, train_loss=0.085, test_acc=0.846\n",
      "epoch=87, train_loss=0.084, test_acc=0.851\n",
      "epoch=88, train_loss=0.083, test_acc=0.791\n",
      "epoch=89, train_loss=0.082, test_acc=0.799\n",
      "epoch=90, train_loss=0.028, test_acc=0.899\n",
      "epoch=91, train_loss=0.006, test_acc=0.903\n",
      "epoch=92, train_loss=-0.002, test_acc=0.906\n",
      "epoch=93, train_loss=-0.006, test_acc=0.907\n",
      "epoch=94, train_loss=-0.011, test_acc=0.907\n",
      "epoch=95, train_loss=-0.014, test_acc=0.908\n",
      "epoch=96, train_loss=-0.016, test_acc=0.907\n",
      "epoch=97, train_loss=-0.019, test_acc=0.908\n",
      "epoch=98, train_loss=-0.020, test_acc=0.906\n",
      "epoch=99, train_loss=-0.022, test_acc=0.906\n",
      "epoch=100, train_loss=-0.025, test_acc=0.905\n",
      "epoch=101, train_loss=-0.027, test_acc=0.905\n",
      "epoch=102, train_loss=-0.029, test_acc=0.905\n",
      "epoch=103, train_loss=-0.030, test_acc=0.906\n",
      "epoch=104, train_loss=-0.031, test_acc=0.904\n",
      "epoch=105, train_loss=-0.033, test_acc=0.905\n",
      "epoch=106, train_loss=-0.033, test_acc=0.903\n",
      "epoch=107, train_loss=-0.034, test_acc=0.904\n",
      "epoch=108, train_loss=-0.036, test_acc=0.902\n",
      "epoch=109, train_loss=-0.036, test_acc=0.904\n",
      "epoch=110, train_loss=-0.037, test_acc=0.904\n",
      "epoch=111, train_loss=-0.039, test_acc=0.902\n",
      "epoch=112, train_loss=-0.040, test_acc=0.904\n",
      "epoch=113, train_loss=-0.042, test_acc=0.904\n",
      "epoch=114, train_loss=-0.042, test_acc=0.903\n",
      "epoch=115, train_loss=-0.042, test_acc=0.903\n",
      "epoch=116, train_loss=-0.041, test_acc=0.903\n",
      "epoch=117, train_loss=-0.045, test_acc=0.903\n",
      "epoch=118, train_loss=-0.042, test_acc=0.902\n",
      "epoch=119, train_loss=-0.044, test_acc=0.904\n",
      "epoch=120, train_loss=-0.051, test_acc=0.907\n",
      "epoch=121, train_loss=-0.053, test_acc=0.907\n",
      "epoch=122, train_loss=-0.054, test_acc=0.909\n",
      "epoch=123, train_loss=-0.054, test_acc=0.907\n",
      "epoch=124, train_loss=-0.055, test_acc=0.909\n",
      "epoch=125, train_loss=-0.057, test_acc=0.909\n",
      "epoch=126, train_loss=-0.057, test_acc=0.908\n",
      "epoch=127, train_loss=-0.057, test_acc=0.908\n",
      "epoch=128, train_loss=-0.058, test_acc=0.908\n",
      "epoch=129, train_loss=-0.057, test_acc=0.908\n",
      "epoch=130, train_loss=-0.058, test_acc=0.908\n",
      "epoch=131, train_loss=-0.059, test_acc=0.909\n",
      "epoch=132, train_loss=-0.059, test_acc=0.908\n",
      "epoch=133, train_loss=-0.059, test_acc=0.908\n",
      "epoch=134, train_loss=-0.059, test_acc=0.909\n",
      "epoch=135, train_loss=-0.060, test_acc=0.909\n",
      "epoch=136, train_loss=-0.059, test_acc=0.910\n",
      "epoch=137, train_loss=-0.059, test_acc=0.908\n",
      "epoch=138, train_loss=-0.060, test_acc=0.909\n",
      "epoch=139, train_loss=-0.061, test_acc=0.908\n",
      "epoch=140, train_loss=-0.060, test_acc=0.909\n",
      "epoch=141, train_loss=-0.059, test_acc=0.908\n",
      "epoch=142, train_loss=-0.059, test_acc=0.908\n",
      "epoch=143, train_loss=-0.061, test_acc=0.909\n",
      "epoch=144, train_loss=-0.060, test_acc=0.906\n",
      "epoch=145, train_loss=-0.061, test_acc=0.908\n",
      "epoch=146, train_loss=-0.060, test_acc=0.909\n",
      "epoch=147, train_loss=-0.062, test_acc=0.908\n",
      "epoch=148, train_loss=-0.061, test_acc=0.909\n",
      "epoch=149, train_loss=-0.060, test_acc=0.909\n",
      "epoch=150, train_loss=-0.061, test_acc=0.910\n",
      "epoch=151, train_loss=-0.061, test_acc=0.910\n",
      "epoch=152, train_loss=-0.061, test_acc=0.909\n",
      "epoch=153, train_loss=-0.062, test_acc=0.909\n",
      "epoch=154, train_loss=-0.062, test_acc=0.909\n",
      "epoch=155, train_loss=-0.061, test_acc=0.909\n",
      "epoch=156, train_loss=-0.062, test_acc=0.909\n",
      "epoch=157, train_loss=-0.063, test_acc=0.908\n",
      "epoch=158, train_loss=-0.063, test_acc=0.909\n",
      "epoch=159, train_loss=-0.063, test_acc=0.909\n",
      "epoch=160, train_loss=-0.063, test_acc=0.908\n",
      "epoch=161, train_loss=-0.062, test_acc=0.908\n",
      "epoch=162, train_loss=-0.062, test_acc=0.909\n",
      "epoch=163, train_loss=-0.063, test_acc=0.908\n",
      "epoch=164, train_loss=-0.063, test_acc=0.908\n",
      "epoch=165, train_loss=-0.064, test_acc=0.909\n",
      "epoch=166, train_loss=-0.064, test_acc=0.909\n",
      "epoch=167, train_loss=-0.063, test_acc=0.909\n",
      "epoch=168, train_loss=-0.064, test_acc=0.909\n",
      "epoch=169, train_loss=-0.063, test_acc=0.908\n",
      "epoch=170, train_loss=-0.063, test_acc=0.909\n",
      "epoch=171, train_loss=-0.065, test_acc=0.909\n",
      "epoch=172, train_loss=-0.063, test_acc=0.908\n",
      "epoch=173, train_loss=-0.064, test_acc=0.910\n",
      "epoch=174, train_loss=-0.064, test_acc=0.909\n",
      "epoch=175, train_loss=-0.065, test_acc=0.909\n",
      "epoch=176, train_loss=-0.065, test_acc=0.909\n",
      "epoch=177, train_loss=-0.065, test_acc=0.908\n",
      "epoch=178, train_loss=-0.064, test_acc=0.908\n",
      "epoch=179, train_loss=-0.064, test_acc=0.909\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(params=model.parameters(), lr=0.1, nesterov=True, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = dict(train_loss=[], test_acc=[], train_time=[], test_time=[])\n",
    "for ep in range(epoch):\n",
    "    # train phase\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    s_time = time.time()\n",
    "    for image, target in train_loader:\n",
    "        image = image.to(device)\n",
    "        target = f.one_hot(target, 10).float().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        t_pred = t_model(image)\n",
    "        distill_loss = f.kl_div(torch.softmax(pred / temp, dim=1), torch.softmax(t_pred / temp, dim=1))\n",
    "        student_loss = criterion(pred, target)\n",
    "        loss = alpha * distill_loss + (1-alpha) * student_loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    e_time = time.time()\n",
    "    history['train_loss'].append(train_loss/len(train_loader))\n",
    "    history['train_time'].append(e_time - s_time)\n",
    "\n",
    "    # test phase\n",
    "    test_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    s_time = time.time()\n",
    "    for image, target in test_loader:\n",
    "        image = image.to(device)\n",
    "        target = f.one_hot(target, 10).float().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        test_acc += torch.sum(torch.argmax(pred, dim=1) == torch.argmax(target, dim=1)).item()\n",
    "    e_time = time.time()\n",
    "    history['test_acc'].append(test_acc/len(test_dataset))\n",
    "    history['test_time'].append(e_time - s_time)\n",
    "    print(f'epoch={ep}, train_loss={train_loss/len(train_loader):.3f}, test_acc={test_acc/len(test_dataset):.3f}')\n",
    "\n",
    "    checkpoint = dict(\n",
    "        model=model.state_dict(),\n",
    "        optimizer=optimizer.state_dict(),\n",
    "        history=history,\n",
    "        epoch=ep\n",
    "    )\n",
    "    torch.save(checkpoint, f'./result/{model_name}.pt')\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
