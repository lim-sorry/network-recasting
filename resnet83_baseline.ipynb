{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import matplotlib as plt\n",
    "import time\n",
    "\n",
    "from model import ConvBlock, CustomResNet, initialize_weights\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import ToTensor, Compose, RandomCrop, RandomHorizontalFlip, Normalize\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "epoch = 180\n",
    "gamma = 0.1\n",
    "milestones = [90, 120] # from resnet paper\n",
    "temp = 5\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(size=[32, 32], padding=4),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 869530\n"
     ]
    }
   ],
   "source": [
    "model_name = 'resnet83_baseline'\n",
    "\n",
    "model = CustomResNet(block=Bottleneck,\n",
    "                   layers=[9, 9, 9],\n",
    "                   num_classes=10).to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\js-win-lab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=2.403, test_acc=0.278\n",
      "epoch=1, train_loss=1.805, test_acc=0.332\n",
      "epoch=2, train_loss=1.611, test_acc=0.421\n",
      "epoch=3, train_loss=1.425, test_acc=0.519\n",
      "epoch=4, train_loss=1.253, test_acc=0.562\n",
      "epoch=5, train_loss=1.109, test_acc=0.629\n",
      "epoch=6, train_loss=1.003, test_acc=0.629\n",
      "epoch=7, train_loss=0.914, test_acc=0.658\n",
      "epoch=8, train_loss=0.833, test_acc=0.643\n",
      "epoch=9, train_loss=0.762, test_acc=0.726\n",
      "epoch=10, train_loss=0.706, test_acc=0.744\n",
      "epoch=11, train_loss=0.654, test_acc=0.748\n",
      "epoch=12, train_loss=0.617, test_acc=0.732\n",
      "epoch=13, train_loss=0.583, test_acc=0.778\n",
      "epoch=14, train_loss=0.551, test_acc=0.755\n",
      "epoch=15, train_loss=0.521, test_acc=0.775\n",
      "epoch=16, train_loss=0.501, test_acc=0.797\n",
      "epoch=17, train_loss=0.479, test_acc=0.806\n",
      "epoch=18, train_loss=0.455, test_acc=0.810\n",
      "epoch=19, train_loss=0.444, test_acc=0.785\n",
      "epoch=20, train_loss=0.426, test_acc=0.789\n",
      "epoch=21, train_loss=0.410, test_acc=0.806\n",
      "epoch=22, train_loss=0.403, test_acc=0.833\n",
      "epoch=23, train_loss=0.391, test_acc=0.818\n",
      "epoch=24, train_loss=0.373, test_acc=0.825\n",
      "epoch=25, train_loss=0.364, test_acc=0.834\n",
      "epoch=26, train_loss=0.353, test_acc=0.837\n",
      "epoch=27, train_loss=0.351, test_acc=0.841\n",
      "epoch=28, train_loss=0.338, test_acc=0.842\n",
      "epoch=29, train_loss=0.333, test_acc=0.830\n",
      "epoch=30, train_loss=0.328, test_acc=0.826\n",
      "epoch=31, train_loss=0.323, test_acc=0.845\n",
      "epoch=32, train_loss=0.317, test_acc=0.834\n",
      "epoch=33, train_loss=0.308, test_acc=0.820\n",
      "epoch=34, train_loss=0.305, test_acc=0.857\n",
      "epoch=35, train_loss=0.298, test_acc=0.854\n",
      "epoch=36, train_loss=0.296, test_acc=0.867\n",
      "epoch=37, train_loss=0.286, test_acc=0.866\n",
      "epoch=38, train_loss=0.281, test_acc=0.819\n",
      "epoch=39, train_loss=0.280, test_acc=0.863\n",
      "epoch=40, train_loss=0.275, test_acc=0.862\n",
      "epoch=41, train_loss=0.269, test_acc=0.857\n",
      "epoch=42, train_loss=0.269, test_acc=0.858\n",
      "epoch=43, train_loss=0.264, test_acc=0.839\n",
      "epoch=44, train_loss=0.262, test_acc=0.849\n",
      "epoch=45, train_loss=0.262, test_acc=0.871\n",
      "epoch=46, train_loss=0.258, test_acc=0.872\n",
      "epoch=47, train_loss=0.255, test_acc=0.843\n",
      "epoch=48, train_loss=0.250, test_acc=0.847\n",
      "epoch=49, train_loss=0.246, test_acc=0.847\n",
      "epoch=50, train_loss=0.243, test_acc=0.872\n",
      "epoch=51, train_loss=0.244, test_acc=0.854\n",
      "epoch=52, train_loss=0.240, test_acc=0.841\n",
      "epoch=53, train_loss=0.240, test_acc=0.874\n",
      "epoch=54, train_loss=0.239, test_acc=0.858\n",
      "epoch=55, train_loss=0.236, test_acc=0.859\n",
      "epoch=56, train_loss=0.230, test_acc=0.889\n",
      "epoch=57, train_loss=0.227, test_acc=0.878\n",
      "epoch=58, train_loss=0.224, test_acc=0.874\n",
      "epoch=59, train_loss=0.228, test_acc=0.873\n",
      "epoch=60, train_loss=0.223, test_acc=0.881\n",
      "epoch=61, train_loss=0.219, test_acc=0.875\n",
      "epoch=62, train_loss=0.218, test_acc=0.852\n",
      "epoch=63, train_loss=0.220, test_acc=0.884\n",
      "epoch=64, train_loss=0.212, test_acc=0.869\n",
      "epoch=65, train_loss=0.217, test_acc=0.865\n",
      "epoch=66, train_loss=0.215, test_acc=0.878\n",
      "epoch=67, train_loss=0.213, test_acc=0.849\n",
      "epoch=68, train_loss=0.209, test_acc=0.864\n",
      "epoch=69, train_loss=0.207, test_acc=0.868\n",
      "epoch=70, train_loss=0.211, test_acc=0.867\n",
      "epoch=71, train_loss=0.207, test_acc=0.876\n",
      "epoch=72, train_loss=0.204, test_acc=0.882\n",
      "epoch=73, train_loss=0.202, test_acc=0.875\n",
      "epoch=74, train_loss=0.202, test_acc=0.883\n",
      "epoch=75, train_loss=0.208, test_acc=0.868\n",
      "epoch=76, train_loss=0.200, test_acc=0.866\n",
      "epoch=77, train_loss=0.200, test_acc=0.872\n",
      "epoch=78, train_loss=0.195, test_acc=0.889\n",
      "epoch=79, train_loss=0.195, test_acc=0.871\n",
      "epoch=80, train_loss=0.198, test_acc=0.891\n",
      "epoch=81, train_loss=0.191, test_acc=0.870\n",
      "epoch=82, train_loss=0.196, test_acc=0.889\n",
      "epoch=83, train_loss=0.191, test_acc=0.880\n",
      "epoch=84, train_loss=0.198, test_acc=0.873\n",
      "epoch=85, train_loss=0.188, test_acc=0.879\n",
      "epoch=86, train_loss=0.187, test_acc=0.893\n",
      "epoch=87, train_loss=0.189, test_acc=0.884\n",
      "epoch=88, train_loss=0.188, test_acc=0.884\n",
      "epoch=89, train_loss=0.187, test_acc=0.876\n",
      "epoch=90, train_loss=0.104, test_acc=0.927\n",
      "epoch=91, train_loss=0.074, test_acc=0.930\n",
      "epoch=92, train_loss=0.063, test_acc=0.931\n",
      "epoch=93, train_loss=0.058, test_acc=0.930\n",
      "epoch=94, train_loss=0.051, test_acc=0.931\n",
      "epoch=95, train_loss=0.048, test_acc=0.931\n",
      "epoch=96, train_loss=0.044, test_acc=0.935\n",
      "epoch=97, train_loss=0.040, test_acc=0.932\n",
      "epoch=98, train_loss=0.038, test_acc=0.932\n",
      "epoch=99, train_loss=0.035, test_acc=0.933\n",
      "epoch=100, train_loss=0.034, test_acc=0.932\n",
      "epoch=101, train_loss=0.032, test_acc=0.932\n",
      "epoch=102, train_loss=0.028, test_acc=0.933\n",
      "epoch=103, train_loss=0.028, test_acc=0.934\n",
      "epoch=104, train_loss=0.028, test_acc=0.932\n",
      "epoch=105, train_loss=0.025, test_acc=0.932\n",
      "epoch=106, train_loss=0.026, test_acc=0.934\n",
      "epoch=107, train_loss=0.023, test_acc=0.932\n",
      "epoch=108, train_loss=0.023, test_acc=0.933\n",
      "epoch=109, train_loss=0.022, test_acc=0.933\n",
      "epoch=110, train_loss=0.020, test_acc=0.932\n",
      "epoch=111, train_loss=0.020, test_acc=0.934\n",
      "epoch=112, train_loss=0.019, test_acc=0.933\n",
      "epoch=113, train_loss=0.018, test_acc=0.934\n",
      "epoch=114, train_loss=0.017, test_acc=0.934\n",
      "epoch=115, train_loss=0.017, test_acc=0.934\n",
      "epoch=116, train_loss=0.017, test_acc=0.932\n",
      "epoch=117, train_loss=0.016, test_acc=0.933\n",
      "epoch=118, train_loss=0.016, test_acc=0.933\n",
      "epoch=119, train_loss=0.015, test_acc=0.932\n",
      "epoch=120, train_loss=0.013, test_acc=0.932\n",
      "epoch=121, train_loss=0.012, test_acc=0.933\n",
      "epoch=122, train_loss=0.011, test_acc=0.932\n",
      "epoch=123, train_loss=0.012, test_acc=0.933\n",
      "epoch=124, train_loss=0.012, test_acc=0.933\n",
      "epoch=125, train_loss=0.012, test_acc=0.934\n",
      "epoch=126, train_loss=0.011, test_acc=0.933\n",
      "epoch=127, train_loss=0.011, test_acc=0.933\n",
      "epoch=128, train_loss=0.011, test_acc=0.934\n",
      "epoch=129, train_loss=0.011, test_acc=0.934\n",
      "epoch=130, train_loss=0.011, test_acc=0.933\n",
      "epoch=131, train_loss=0.011, test_acc=0.934\n",
      "epoch=132, train_loss=0.010, test_acc=0.934\n",
      "epoch=133, train_loss=0.010, test_acc=0.935\n",
      "epoch=134, train_loss=0.010, test_acc=0.935\n",
      "epoch=135, train_loss=0.010, test_acc=0.934\n",
      "epoch=136, train_loss=0.011, test_acc=0.934\n",
      "epoch=137, train_loss=0.010, test_acc=0.933\n",
      "epoch=138, train_loss=0.010, test_acc=0.934\n",
      "epoch=139, train_loss=0.010, test_acc=0.933\n",
      "epoch=140, train_loss=0.010, test_acc=0.934\n",
      "epoch=141, train_loss=0.010, test_acc=0.934\n",
      "epoch=142, train_loss=0.009, test_acc=0.933\n",
      "epoch=143, train_loss=0.009, test_acc=0.934\n",
      "epoch=144, train_loss=0.009, test_acc=0.933\n",
      "epoch=145, train_loss=0.010, test_acc=0.934\n",
      "epoch=146, train_loss=0.010, test_acc=0.934\n",
      "epoch=147, train_loss=0.010, test_acc=0.934\n",
      "epoch=148, train_loss=0.009, test_acc=0.933\n",
      "epoch=149, train_loss=0.010, test_acc=0.933\n",
      "epoch=150, train_loss=0.009, test_acc=0.933\n",
      "epoch=151, train_loss=0.009, test_acc=0.934\n",
      "epoch=152, train_loss=0.009, test_acc=0.933\n",
      "epoch=153, train_loss=0.009, test_acc=0.933\n",
      "epoch=154, train_loss=0.009, test_acc=0.934\n",
      "epoch=155, train_loss=0.009, test_acc=0.934\n",
      "epoch=156, train_loss=0.009, test_acc=0.933\n",
      "epoch=157, train_loss=0.009, test_acc=0.935\n",
      "epoch=158, train_loss=0.009, test_acc=0.933\n",
      "epoch=159, train_loss=0.009, test_acc=0.933\n",
      "epoch=160, train_loss=0.008, test_acc=0.933\n",
      "epoch=161, train_loss=0.009, test_acc=0.933\n",
      "epoch=162, train_loss=0.009, test_acc=0.933\n",
      "epoch=163, train_loss=0.008, test_acc=0.933\n",
      "epoch=164, train_loss=0.008, test_acc=0.933\n",
      "epoch=165, train_loss=0.008, test_acc=0.933\n",
      "epoch=166, train_loss=0.008, test_acc=0.934\n",
      "epoch=167, train_loss=0.009, test_acc=0.934\n",
      "epoch=168, train_loss=0.008, test_acc=0.934\n",
      "epoch=169, train_loss=0.009, test_acc=0.934\n",
      "epoch=170, train_loss=0.008, test_acc=0.935\n",
      "epoch=171, train_loss=0.008, test_acc=0.934\n",
      "epoch=172, train_loss=0.008, test_acc=0.934\n",
      "epoch=173, train_loss=0.008, test_acc=0.933\n",
      "epoch=174, train_loss=0.008, test_acc=0.933\n",
      "epoch=175, train_loss=0.008, test_acc=0.935\n",
      "epoch=176, train_loss=0.008, test_acc=0.934\n",
      "epoch=177, train_loss=0.008, test_acc=0.934\n",
      "epoch=178, train_loss=0.008, test_acc=0.934\n",
      "epoch=179, train_loss=0.008, test_acc=0.934\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(params=model.parameters(), lr=0.1, nesterov=True, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = dict(train_loss=[], test_acc=[], train_time=[], test_time=[])\n",
    "for ep in range(epoch):\n",
    "    # train phase\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    s_time = time.time()\n",
    "    for image, target in train_loader:\n",
    "        image = image.to(device)\n",
    "        target = f.one_hot(target, 10).float().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    e_time = time.time()\n",
    "    history['train_loss'].append(train_loss/len(train_loader))\n",
    "    history['train_time'].append(e_time - s_time)\n",
    "\n",
    "    # test phase\n",
    "    test_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    s_time = time.time()\n",
    "    for image, target in test_loader:\n",
    "        image = image.to(device)\n",
    "        target = f.one_hot(target, 10).float().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        test_acc += torch.sum(torch.argmax(pred, dim=1) == torch.argmax(target, dim=1)).item()\n",
    "    e_time = time.time()\n",
    "    history['test_acc'].append(test_acc/len(test_dataset))\n",
    "    history['test_time'].append(e_time - s_time)\n",
    "    print(f'epoch={ep}, train_loss={train_loss/len(train_loader):.3f}, test_acc={test_acc/len(test_dataset):.3f}')\n",
    "\n",
    "    checkpoint = dict(\n",
    "        model=model.state_dict(),\n",
    "        optimizer=optimizer.state_dict(),\n",
    "        history=history,\n",
    "        epoch=ep\n",
    "    )\n",
    "    torch.save(checkpoint, f'./result/{model_name}.pt')\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
